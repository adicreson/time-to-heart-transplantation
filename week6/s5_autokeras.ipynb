{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"../Data/m4_imputed.csv\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median waiting list time: 88.0\n",
      "1    0.501483\n",
      "0    0.498517\n",
      "Name: wl_time, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "median_wl_time = df['wl_time'].median()\n",
    "print(f\"Median waiting list time: {median_wl_time}\")\n",
    "\n",
    "df['wl_time'] = df['wl_time'].apply(lambda x: 0 if x < median_wl_time else 1)\n",
    "print(df['wl_time'].value_counts(normalize=True))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_score(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 15 Complete [00h 00m 59s]\n",
      "val_accuracy: 0.6980460286140442\n",
      "\n",
      "Best val_accuracy So Far: 0.7017999887466431\n",
      "Total elapsed time: 01h 13m 25s\n",
      "\n",
      "Search: Running Trial #16\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "True              |True              |structured_data_block_1/normalize\n",
      "False             |False             |structured_data_block_1/dense_block_1/use_batchnorm\n",
      "3                 |3                 |structured_data_block_1/dense_block_1/num_layers\n",
      "64                |64                |structured_data_block_1/dense_block_1/units_0\n",
      "0                 |0                 |structured_data_block_1/dense_block_1/dropout\n",
      "32                |32                |structured_data_block_1/dense_block_1/units_1\n",
      "0                 |0                 |classification_head_1/dropout\n",
      "adam              |adam              |optimizer\n",
      "0.001             |0.001             |learning_rate\n",
      "32                |32                |structured_data_block_1/dense_block_1/units_2\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from autokeras import StructuredDataClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "X = df.drop(columns=['wl_time'])\n",
    "y = df['wl_time']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "clf = StructuredDataClassifier(max_trials=25,\n",
    "                               project_name=\"m4_pre_feat\",\n",
    "                               metrics=[\"accuracy\"],\n",
    "                               overwrite=False)\n",
    "\n",
    "clf.fit(X_train, y_train, verbose=1)\n",
    "print(clf.evaluate(X_test, y_test))\n",
    "f1_s = f1_score(y_test, clf.predict(X_test))\n",
    "print(f\"F1 score: {f1_s}\")\n",
    "print(\"F1 Macro:\", f1_score(y_test, clf.predict(X_test), average='macro'))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
